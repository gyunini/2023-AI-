{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNYIPD53szpuO1xdYsTDyo2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyunini/2023-AI-Intensive-Challange/blob/main/2023_AI_Intensive_Challange.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa-Dq_9FicCR",
        "outputId": "96698f76-0cfd-4f0c-83ae-3fe23e0500fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, models, transforms\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/ColabNotebooks/집중교육1_CV실습/2023-ai-challenge'\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "trainset = 'trainset.npy' # 50,000개의 데이터,\n",
        "trainlabel = 'trainlabel.npy'\n",
        "testset = 'testset.npy'  # 10,000개의 데이터\n",
        "sample_submission = 'sample_submission.csv'\n",
        "trainset = np.load(os.path.join(data_dir, trainset))\n",
        "trainlabel = np.load(os.path.join(data_dir, trainlabel))\n",
        "testset = np.load(os.path.join(data_dir, testset))\n",
        "\n",
        "CIFAR100_CLASSES = sorted(['beaver', 'dolphin', 'otter', 'seal', 'whale',  # aquatic mammals\n",
        "                           'aquarium' 'fish', 'flatfish', 'ray', 'shark', 'trout',  # fish\n",
        "                           'orchids', 'poppies', 'roses', 'sunflowers', 'tulips', # flowers\n",
        "                           'bottles', 'bowls', 'cans', 'cups', 'plates', # food containers\n",
        "                           'apples', 'mushrooms', 'oranges', 'pears', 'sweet peppers', # fruit and vegetables\n",
        "                           'clock', 'computer' 'keyboard', 'lamp', 'telephone', 'television', # household electrical devices\n",
        "                           'bed', 'chair', 'couch', 'table', 'wardrobe', # household furniture\n",
        "                           'bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach', # insects\n",
        "                           'bear', 'leopard', 'lion', 'tiger', 'wolf', # large carnivores\n",
        "                           'bridge', 'castle', 'house', 'road', 'skyscraper', # large man-made outdoor things\n",
        "                           'cloud', 'forest', 'mountain', 'plain', 'sea', # large natural outdoor scenes\n",
        "                           'camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo', # large omnivores and herbivores\n",
        "                           'fox', 'porcupine', 'possum', 'raccoon', 'skunk', # medium-sized mammals\n",
        "                           'crab', 'lobster', 'snail', 'spider', 'worm', # non-insect invertebrates\n",
        "                           'baby', 'boy', 'girl', 'man', 'woman', # people\n",
        "                           'crocodile', 'dinosaur', 'lizard', 'snake', 'turtle', # reptiles\n",
        "                           'hamster', 'mouse', 'rabbit', 'shrew', 'squirrel', # small mammals\n",
        "                           'maple', 'oak', 'palm', 'pine', 'willow', # trees\n",
        "                           'bicycle', 'bus', 'motorcycle', 'pickup truck', 'train', # vehicles 1\n",
        "                           'lawn-mower', 'rocket', 'streetcar', 'tank', 'tractor' # vehicles 2\n",
        "                          ])\n",
        "\n",
        "\n",
        "config = {'Cifar100_stats': [[0.5071, 0.4867, 0.4408],\n",
        "                            [0.2675, 0.2565, 0.2761]],\n",
        "          'batch_size'  : 256,\n",
        "          'worker'      : 2,\n",
        "          'epochs'      : 400,\n",
        "          'momentum'    : 0.9,\n",
        "          'lr_decay'    : 0.05,\n",
        "          'SGD_lr'      : 0.01,\n",
        "          'Adam_lr'     : 0.005,\n",
        "          }\n",
        "\n",
        "print(trainset.shape)\n",
        "# print(trainset[0])\n",
        "print(trainlabel)\n",
        "print(trainset[6].shape)\n",
        "plt.imshow(trainset[6], interpolation='nearest') # 맨 앞의 채널을 날려줘야 이차원 이미지를 보여줄 수 있음\n",
        "plt.title(f'Label : {CIFAR100_CLASSES[trainlabel[6]]}')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "sOzE0RC3imIM",
        "outputId": "0fb4d770-b03a-4a74-f11d-7bb11a616aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "(50000, 32, 32, 3)\n",
            "[23 81 72 ... 35 56 31]\n",
            "(32, 32, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(testset[6].shape)\n",
        "plt.imshow(testset[51], interpolation='nearest') # 맨 앞의 채널을 날려줘야 이차원 이미지를 보여줄 수 있음\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "jBWbj7w1krUF",
        "outputId": "ca82d806-6507-4c68-d30b-54b887042500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 32, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''MobileNetV3 in PyTorch.\n",
        "\n",
        "See the paper \"Inverted Residuals and Linear Bottlenecks:\n",
        "Mobile Networks for Classification, Detection and Segmentation\" for more details.\n",
        "'''\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "\n",
        "\n",
        "\n",
        "class hswish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        out = x * F.relu6(x + 3, inplace=True) / 6\n",
        "        return out\n",
        "\n",
        "\n",
        "class hsigmoid(nn.Module):\n",
        "    def forward(self, x):\n",
        "        out = F.relu6(x + 3, inplace=True) / 6\n",
        "        return out\n",
        "\n",
        "\n",
        "class SeModule(nn.Module):\n",
        "    def __init__(self, in_size, reduction=4):\n",
        "        super(SeModule, self).__init__()\n",
        "        expand_size =  max(in_size // reduction, 8)\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_size, expand_size, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(expand_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(expand_size, in_size, kernel_size=1, bias=False),\n",
        "            nn.Hardsigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.se(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    '''expand + depthwise + pointwise'''\n",
        "    def __init__(self, kernel_size, in_size, expand_size, out_size, act, se, stride):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_size, expand_size, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(expand_size)\n",
        "        self.act1 = act(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(expand_size, expand_size, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, groups=expand_size, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(expand_size)\n",
        "        self.act2 = act(inplace=True)\n",
        "        self.se = SeModule(expand_size) if se else nn.Identity()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(expand_size, out_size, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_size)\n",
        "        self.act3 = act(inplace=True)\n",
        "\n",
        "        self.skip = None\n",
        "        if stride == 1 and in_size != out_size:\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Conv2d(in_size, out_size, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(out_size)\n",
        "            )\n",
        "\n",
        "        if stride == 2 and in_size != out_size:\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=in_size, out_channels=in_size, kernel_size=3, groups=in_size, stride=2, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(in_size),\n",
        "                nn.Conv2d(in_size, out_size, kernel_size=1, bias=True),\n",
        "                nn.BatchNorm2d(out_size)\n",
        "            )\n",
        "\n",
        "        if stride == 2 and in_size == out_size:\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=in_size, out_channels=out_size, kernel_size=3, groups=in_size, stride=2, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(out_size)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = x\n",
        "\n",
        "        out = self.act1(self.bn1(self.conv1(x)))\n",
        "        out = self.act2(self.bn2(self.conv2(out)))\n",
        "        out = self.se(out)\n",
        "        out = self.bn3(self.conv3(out))\n",
        "\n",
        "        if self.skip is not None:\n",
        "            skip = self.skip(skip)\n",
        "        return self.act3(out + skip)\n",
        "\n",
        "\n",
        "\n",
        "class MobileNetV3_Small(nn.Module):\n",
        "    def __init__(self, num_classes=100, act=nn.Hardswish):\n",
        "        super(MobileNetV3_Small, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.hs1 = act(inplace=True)\n",
        "\n",
        "        # self.bneck = nn.Sequential(\n",
        "        #     Block(3, 16, 16, 16, nn.ReLU, True, 2),\n",
        "        #     Block(3, 16, 64, 24, nn.ReLU, False, 1),\n",
        "        #     Block(3, 24, 88, 24, nn.ReLU, False, 1),\n",
        "        #     Block(5, 24, 120, 40, act, True, 2),\n",
        "        #     Block(5, 40, 120, 80, act, True, 1),\n",
        "        #     Block(5, 80, 90, 96, act, True, 1),\n",
        "        # )\n",
        "        # self.bneck = nn.Sequential(\n",
        "        #     Block(3, 16, 16, 16, nn.ReLU, True, 2),\n",
        "        #     Block(3, 16, 64, 24, nn.ReLU, False, 1),\n",
        "        #     Block(3, 24, 120, 40, nn.ReLU, False, 1),\n",
        "        #     Block(5, 40, 120, 80, act, True, 2),\n",
        "        #     Block(5, 80, 120, 112, act, True, 1),\n",
        "        #     # Block(5, 80, 90, 96, act, True, 1),\n",
        "        # )\n",
        "        self.bneck = nn.Sequential(\n",
        "            Block(3, 16, 16, 16, nn.ReLU, True, 2),\n",
        "            Block(3, 16, 64, 24, nn.ReLU, False, 1),\n",
        "            Block(3, 24, 120, 24, nn.ReLU, False, 1),\n",
        "            Block(5, 24, 144, 40, act, True, 2),\n",
        "            Block(5, 40, 120, 65, act, True, 1),\n",
        "            Block(5, 65, 80, 96, act, True, 1),\n",
        "        )\n",
        "\n",
        "\n",
        "        # self.conv2 = nn.Conv2d(112, 256, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        # self.bn2 = nn.BatchNorm2d(256)\n",
        "        # self.hs2 = act(inplace=True)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # self.linear3 = nn.Linear(576, 1280, bias=False)\n",
        "        # self.bn3 = nn.BatchNorm1d(96)\n",
        "        # self.hs3 = act(inplace=True)\n",
        "        # self.drop = nn.Dropout(0.2)\n",
        "        self.linear4 = nn.Linear(96, num_classes)\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                init.constant_(m.weight, 1)\n",
        "                init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.normal_(m.weight, std=0.001)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.hs1(self.bn1(self.conv1(x)))\n",
        "        out = self.bneck(out)\n",
        "\n",
        "        # out = self.hs2(self.bn2(self.conv2(out)))\n",
        "        x = self.gap(out)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # out = self.gap(out).flatten(1)\n",
        "        # out = self.drop(self.hs3(self.bn3(self.linear3(out))))\n",
        "        # x = self.bn3(x)\n",
        "        return self.linear4(x)"
      ],
      "metadata": {
        "id": "VHMCZU8XioEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenetv3_small = MobileNetV3_Small()\n",
        "mobilenetv3_small.to(device)\n",
        "summary(mobilenetv3_small, (3,32,32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_JcXpmyjwr9",
        "outputId": "4daa1d6c-63d4-4bcb-e6d1-4c813bb9ec89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             432\n",
            "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
            "         Hardswish-3           [-1, 16, 32, 32]               0\n",
            "            Conv2d-4           [-1, 16, 32, 32]             256\n",
            "       BatchNorm2d-5           [-1, 16, 32, 32]              32\n",
            "              ReLU-6           [-1, 16, 32, 32]               0\n",
            "            Conv2d-7           [-1, 16, 16, 16]             144\n",
            "       BatchNorm2d-8           [-1, 16, 16, 16]              32\n",
            "              ReLU-9           [-1, 16, 16, 16]               0\n",
            "AdaptiveAvgPool2d-10             [-1, 16, 1, 1]               0\n",
            "           Conv2d-11              [-1, 8, 1, 1]             128\n",
            "      BatchNorm2d-12              [-1, 8, 1, 1]              16\n",
            "             ReLU-13              [-1, 8, 1, 1]               0\n",
            "           Conv2d-14             [-1, 16, 1, 1]             128\n",
            "      Hardsigmoid-15             [-1, 16, 1, 1]               0\n",
            "         SeModule-16           [-1, 16, 16, 16]               0\n",
            "           Conv2d-17           [-1, 16, 16, 16]             256\n",
            "      BatchNorm2d-18           [-1, 16, 16, 16]              32\n",
            "           Conv2d-19           [-1, 16, 16, 16]             144\n",
            "      BatchNorm2d-20           [-1, 16, 16, 16]              32\n",
            "             ReLU-21           [-1, 16, 16, 16]               0\n",
            "            Block-22           [-1, 16, 16, 16]               0\n",
            "           Conv2d-23           [-1, 64, 16, 16]           1,024\n",
            "      BatchNorm2d-24           [-1, 64, 16, 16]             128\n",
            "             ReLU-25           [-1, 64, 16, 16]               0\n",
            "           Conv2d-26           [-1, 64, 16, 16]             576\n",
            "      BatchNorm2d-27           [-1, 64, 16, 16]             128\n",
            "             ReLU-28           [-1, 64, 16, 16]               0\n",
            "         Identity-29           [-1, 64, 16, 16]               0\n",
            "           Conv2d-30           [-1, 24, 16, 16]           1,536\n",
            "      BatchNorm2d-31           [-1, 24, 16, 16]              48\n",
            "           Conv2d-32           [-1, 24, 16, 16]             384\n",
            "      BatchNorm2d-33           [-1, 24, 16, 16]              48\n",
            "             ReLU-34           [-1, 24, 16, 16]               0\n",
            "            Block-35           [-1, 24, 16, 16]               0\n",
            "           Conv2d-36          [-1, 120, 16, 16]           2,880\n",
            "      BatchNorm2d-37          [-1, 120, 16, 16]             240\n",
            "             ReLU-38          [-1, 120, 16, 16]               0\n",
            "           Conv2d-39          [-1, 120, 16, 16]           1,080\n",
            "      BatchNorm2d-40          [-1, 120, 16, 16]             240\n",
            "             ReLU-41          [-1, 120, 16, 16]               0\n",
            "         Identity-42          [-1, 120, 16, 16]               0\n",
            "           Conv2d-43           [-1, 24, 16, 16]           2,880\n",
            "      BatchNorm2d-44           [-1, 24, 16, 16]              48\n",
            "             ReLU-45           [-1, 24, 16, 16]               0\n",
            "            Block-46           [-1, 24, 16, 16]               0\n",
            "           Conv2d-47          [-1, 144, 16, 16]           3,456\n",
            "      BatchNorm2d-48          [-1, 144, 16, 16]             288\n",
            "        Hardswish-49          [-1, 144, 16, 16]               0\n",
            "           Conv2d-50            [-1, 144, 8, 8]           3,600\n",
            "      BatchNorm2d-51            [-1, 144, 8, 8]             288\n",
            "        Hardswish-52            [-1, 144, 8, 8]               0\n",
            "AdaptiveAvgPool2d-53            [-1, 144, 1, 1]               0\n",
            "           Conv2d-54             [-1, 36, 1, 1]           5,184\n",
            "      BatchNorm2d-55             [-1, 36, 1, 1]              72\n",
            "             ReLU-56             [-1, 36, 1, 1]               0\n",
            "           Conv2d-57            [-1, 144, 1, 1]           5,184\n",
            "      Hardsigmoid-58            [-1, 144, 1, 1]               0\n",
            "         SeModule-59            [-1, 144, 8, 8]               0\n",
            "           Conv2d-60             [-1, 40, 8, 8]           5,760\n",
            "      BatchNorm2d-61             [-1, 40, 8, 8]              80\n",
            "           Conv2d-62             [-1, 24, 8, 8]             216\n",
            "      BatchNorm2d-63             [-1, 24, 8, 8]              48\n",
            "           Conv2d-64             [-1, 40, 8, 8]           1,000\n",
            "      BatchNorm2d-65             [-1, 40, 8, 8]              80\n",
            "        Hardswish-66             [-1, 40, 8, 8]               0\n",
            "            Block-67             [-1, 40, 8, 8]               0\n",
            "           Conv2d-68            [-1, 120, 8, 8]           4,800\n",
            "      BatchNorm2d-69            [-1, 120, 8, 8]             240\n",
            "        Hardswish-70            [-1, 120, 8, 8]               0\n",
            "           Conv2d-71            [-1, 120, 8, 8]           3,000\n",
            "      BatchNorm2d-72            [-1, 120, 8, 8]             240\n",
            "        Hardswish-73            [-1, 120, 8, 8]               0\n",
            "AdaptiveAvgPool2d-74            [-1, 120, 1, 1]               0\n",
            "           Conv2d-75             [-1, 30, 1, 1]           3,600\n",
            "      BatchNorm2d-76             [-1, 30, 1, 1]              60\n",
            "             ReLU-77             [-1, 30, 1, 1]               0\n",
            "           Conv2d-78            [-1, 120, 1, 1]           3,600\n",
            "      Hardsigmoid-79            [-1, 120, 1, 1]               0\n",
            "         SeModule-80            [-1, 120, 8, 8]               0\n",
            "           Conv2d-81             [-1, 65, 8, 8]           7,800\n",
            "      BatchNorm2d-82             [-1, 65, 8, 8]             130\n",
            "           Conv2d-83             [-1, 65, 8, 8]           2,600\n",
            "      BatchNorm2d-84             [-1, 65, 8, 8]             130\n",
            "        Hardswish-85             [-1, 65, 8, 8]               0\n",
            "            Block-86             [-1, 65, 8, 8]               0\n",
            "           Conv2d-87             [-1, 80, 8, 8]           5,200\n",
            "      BatchNorm2d-88             [-1, 80, 8, 8]             160\n",
            "        Hardswish-89             [-1, 80, 8, 8]               0\n",
            "           Conv2d-90             [-1, 80, 8, 8]           2,000\n",
            "      BatchNorm2d-91             [-1, 80, 8, 8]             160\n",
            "        Hardswish-92             [-1, 80, 8, 8]               0\n",
            "AdaptiveAvgPool2d-93             [-1, 80, 1, 1]               0\n",
            "           Conv2d-94             [-1, 20, 1, 1]           1,600\n",
            "      BatchNorm2d-95             [-1, 20, 1, 1]              40\n",
            "             ReLU-96             [-1, 20, 1, 1]               0\n",
            "           Conv2d-97             [-1, 80, 1, 1]           1,600\n",
            "      Hardsigmoid-98             [-1, 80, 1, 1]               0\n",
            "         SeModule-99             [-1, 80, 8, 8]               0\n",
            "          Conv2d-100             [-1, 96, 8, 8]           7,680\n",
            "     BatchNorm2d-101             [-1, 96, 8, 8]             192\n",
            "          Conv2d-102             [-1, 96, 8, 8]           6,240\n",
            "     BatchNorm2d-103             [-1, 96, 8, 8]             192\n",
            "       Hardswish-104             [-1, 96, 8, 8]               0\n",
            "           Block-105             [-1, 96, 8, 8]               0\n",
            "AdaptiveAvgPool2d-106             [-1, 96, 1, 1]               0\n",
            "          Linear-107                  [-1, 100]           9,700\n",
            "================================================================\n",
            "Total params: 99,124\n",
            "Trainable params: 99,124\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 6.48\n",
            "Params size (MB): 0.38\n",
            "Estimated Total Size (MB): 6.87\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR100Dataset(Dataset): # 커스텀 데이터셋 구성을 위해 Dataset 상속 후 __init__, __len__, __getitem__ 오버라이딩\n",
        "    \"\"\"custom cifar100 dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, initial_data, label, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            initial_data: all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.feature = initial_data\n",
        "        self.label = label\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.feature)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.feature[idx]\n",
        "        label = self.label[idx]\n",
        "        # label = torch.from_numpy(np.array(label))\n",
        "        if self.transform: # transform 있으면 적용\n",
        "            sample = self.transform(sample)\n",
        "        return sample, label"
      ],
      "metadata": {
        "id": "QpWghvl1knML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = CIFAR100Dataset(initial_data=trainset, label=trainlabel, transform=transforms.Compose([ # 위의 코드와는 다르게 transform이 파라미터로 들어감\n",
        "                                               transforms.ToTensor(),\n",
        "                                              #  transforms.Resize(256, antialias=False),\n",
        "                                              transforms.RandomHorizontalFlip(),\n",
        "                                              transforms.ColorJitter(brightness=0.5),\n",
        "                                               transforms.Normalize(*config['Cifar100_stats']), # mean = Cifar10_stats[0], std = Cifar10_stats[1]\n",
        "                                           ]))"
      ],
      "metadata": {
        "id": "QR41Xx75nAQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loader\n",
        "trainloader = DataLoader(trainset, batch_size = config['batch_size'], shuffle = True, num_workers = config['worker'])"
      ],
      "metadata": {
        "id": "v_gtIZF7nCKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path = os.path.join(data_dir, 'mobilenetv3_small_plz_1014.pth')\n",
        "# mobilenetv3_small.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "IZtOQ9e2FASC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(params=mobilenetv3_small.parameters(), lr = config['Adam_lr'], weight_decay=config['lr_decay'])\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, config['Adam_lr'], total_steps=config['epochs'] * len(trainloader))"
      ],
      "metadata": {
        "id": "6ktCS7jTnEDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def onehot(label, n_classes):\n",
        "    return torch.zeros(label.size(0), n_classes).to(device).scatter_(\n",
        "        1, label.view(-1, 1), 1)\n",
        "\n",
        "\n",
        "def mixup(data, targets, alpha=1.0, n_classes=100):\n",
        "    indices = torch.randperm(data.size(0))\n",
        "    data2 = data[indices]\n",
        "    targets2 = targets[indices]\n",
        "\n",
        "    targets = onehot(targets, n_classes)\n",
        "    targets2 = onehot(targets2, n_classes)\n",
        "\n",
        "    lam = torch.FloatTensor([np.random.beta(alpha, alpha)]).to(device)\n",
        "    data = data * lam + data2 * (1 - lam)\n",
        "    targets = targets * lam + targets2 * (1 - lam)\n",
        "\n",
        "    return data, targets\n",
        "\n",
        "def mixup_loss(input, target, size_average=True):\n",
        "    input = F.log_softmax(input, dim=1)\n",
        "    loss = -torch.sum(input * target) # crossentropy_loss\n",
        "    if size_average:\n",
        "        return loss / input.size(0)\n",
        "    else:\n",
        "        return loss\n",
        "\n",
        "# training\n",
        "for epoch in range(400):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in trainloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        inputs = inputs.to(memory_format=torch.channels_last)\n",
        "        inputs, labels = mixup(inputs, labels)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        output = mobilenetv3_small(inputs)\n",
        "        # loss = criterion(output, labels)\n",
        "        loss = mixup_loss(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # train accuracy 계산\n",
        "        # _, predicted = torch.max(output.data, 1)\n",
        "        # total += labels.size(0)\n",
        "        # correct += (predicted == labels).sum().item()\n",
        "\n",
        "        #Time\n",
        "        end_time = time.time()\n",
        "        time_taken = end_time - start_time\n",
        "        time_taken = str(time_taken/60).split('.')\n",
        "\n",
        "    # train_accuracy = 100 * correct / total\n",
        "    # print('Epoch: {}/{}, train_loss: {:.4f}, train_accuracy: {:.2f}%, time:{}m {}s'.format(epoch + 1, 100, running_loss / len(trainset), train_accuracy, time_taken[0], time_taken[1][:2]))\n",
        "    print('Epoch: {}/{}, train_loss: {:.4f}, time:{}m {}s'.format(epoch + 1, 400, running_loss / len(trainset), time_taken[0], time_taken[1][:2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05SxwmeplvGv",
        "outputId": "ee56a330-704f-4212-f617-f7a25bee7105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/400, train_loss: 0.0177, time:0m 18s\n",
            "Epoch: 2/400, train_loss: 0.0171, time:0m 16s\n",
            "Epoch: 3/400, train_loss: 0.0165, time:0m 16s\n",
            "Epoch: 4/400, train_loss: 0.0161, time:0m 17s\n",
            "Epoch: 5/400, train_loss: 0.0158, time:0m 17s\n",
            "Epoch: 6/400, train_loss: 0.0156, time:0m 17s\n",
            "Epoch: 7/400, train_loss: 0.0152, time:0m 17s\n",
            "Epoch: 8/400, train_loss: 0.0152, time:0m 17s\n",
            "Epoch: 9/400, train_loss: 0.0148, time:0m 17s\n",
            "Epoch: 10/400, train_loss: 0.0149, time:0m 17s\n",
            "Epoch: 11/400, train_loss: 0.0147, time:0m 17s\n",
            "Epoch: 12/400, train_loss: 0.0145, time:0m 17s\n",
            "Epoch: 13/400, train_loss: 0.0144, time:0m 17s\n",
            "Epoch: 14/400, train_loss: 0.0141, time:0m 17s\n",
            "Epoch: 15/400, train_loss: 0.0140, time:0m 17s\n",
            "Epoch: 16/400, train_loss: 0.0138, time:0m 17s\n",
            "Epoch: 17/400, train_loss: 0.0137, time:0m 17s\n",
            "Epoch: 18/400, train_loss: 0.0136, time:0m 17s\n",
            "Epoch: 19/400, train_loss: 0.0135, time:0m 17s\n",
            "Epoch: 20/400, train_loss: 0.0136, time:0m 17s\n",
            "Epoch: 21/400, train_loss: 0.0132, time:0m 17s\n",
            "Epoch: 22/400, train_loss: 0.0132, time:0m 17s\n",
            "Epoch: 23/400, train_loss: 0.0131, time:0m 17s\n",
            "Epoch: 24/400, train_loss: 0.0127, time:0m 17s\n",
            "Epoch: 25/400, train_loss: 0.0128, time:0m 17s\n",
            "Epoch: 26/400, train_loss: 0.0127, time:0m 17s\n",
            "Epoch: 27/400, train_loss: 0.0126, time:0m 17s\n",
            "Epoch: 28/400, train_loss: 0.0125, time:0m 17s\n",
            "Epoch: 29/400, train_loss: 0.0125, time:0m 17s\n",
            "Epoch: 30/400, train_loss: 0.0124, time:0m 17s\n",
            "Epoch: 31/400, train_loss: 0.0123, time:0m 17s\n",
            "Epoch: 32/400, train_loss: 0.0120, time:0m 17s\n",
            "Epoch: 33/400, train_loss: 0.0121, time:0m 17s\n",
            "Epoch: 34/400, train_loss: 0.0119, time:0m 17s\n",
            "Epoch: 35/400, train_loss: 0.0118, time:0m 17s\n",
            "Epoch: 36/400, train_loss: 0.0118, time:0m 17s\n",
            "Epoch: 37/400, train_loss: 0.0118, time:0m 17s\n",
            "Epoch: 38/400, train_loss: 0.0116, time:0m 17s\n",
            "Epoch: 39/400, train_loss: 0.0115, time:0m 17s\n",
            "Epoch: 40/400, train_loss: 0.0114, time:0m 17s\n",
            "Epoch: 41/400, train_loss: 0.0114, time:0m 17s\n",
            "Epoch: 42/400, train_loss: 0.0109, time:0m 17s\n",
            "Epoch: 43/400, train_loss: 0.0112, time:0m 17s\n",
            "Epoch: 44/400, train_loss: 0.0112, time:0m 17s\n",
            "Epoch: 45/400, train_loss: 0.0109, time:0m 17s\n",
            "Epoch: 46/400, train_loss: 0.0114, time:0m 17s\n",
            "Epoch: 47/400, train_loss: 0.0112, time:0m 17s\n",
            "Epoch: 48/400, train_loss: 0.0107, time:0m 17s\n",
            "Epoch: 49/400, train_loss: 0.0109, time:0m 17s\n",
            "Epoch: 50/400, train_loss: 0.0110, time:0m 17s\n",
            "Epoch: 51/400, train_loss: 0.0110, time:0m 17s\n",
            "Epoch: 52/400, train_loss: 0.0108, time:0m 17s\n",
            "Epoch: 53/400, train_loss: 0.0110, time:0m 17s\n",
            "Epoch: 54/400, train_loss: 0.0109, time:0m 17s\n",
            "Epoch: 55/400, train_loss: 0.0107, time:0m 17s\n",
            "Epoch: 56/400, train_loss: 0.0107, time:0m 17s\n",
            "Epoch: 57/400, train_loss: 0.0106, time:0m 17s\n",
            "Epoch: 58/400, train_loss: 0.0108, time:0m 17s\n",
            "Epoch: 59/400, train_loss: 0.0107, time:0m 17s\n",
            "Epoch: 60/400, train_loss: 0.0106, time:0m 17s\n",
            "Epoch: 61/400, train_loss: 0.0106, time:0m 17s\n",
            "Epoch: 62/400, train_loss: 0.0104, time:0m 17s\n",
            "Epoch: 63/400, train_loss: 0.0106, time:0m 17s\n",
            "Epoch: 64/400, train_loss: 0.0106, time:0m 17s\n",
            "Epoch: 65/400, train_loss: 0.0106, time:0m 17s\n",
            "Epoch: 66/400, train_loss: 0.0104, time:0m 17s\n",
            "Epoch: 67/400, train_loss: 0.0103, time:0m 17s\n",
            "Epoch: 68/400, train_loss: 0.0105, time:0m 17s\n",
            "Epoch: 69/400, train_loss: 0.0107, time:0m 17s\n",
            "Epoch: 70/400, train_loss: 0.0107, time:0m 17s\n",
            "Epoch: 71/400, train_loss: 0.0105, time:0m 17s\n",
            "Epoch: 72/400, train_loss: 0.0104, time:0m 17s\n",
            "Epoch: 73/400, train_loss: 0.0106, time:0m 17s\n",
            "Epoch: 74/400, train_loss: 0.0104, time:0m 17s\n",
            "Epoch: 75/400, train_loss: 0.0102, time:0m 17s\n",
            "Epoch: 76/400, train_loss: 0.0104, time:0m 17s\n",
            "Epoch: 77/400, train_loss: 0.0105, time:0m 17s\n",
            "Epoch: 78/400, train_loss: 0.0105, time:0m 17s\n",
            "Epoch: 79/400, train_loss: 0.0099, time:0m 17s\n",
            "Epoch: 80/400, train_loss: 0.0102, time:0m 17s\n",
            "Epoch: 81/400, train_loss: 0.0102, time:0m 17s\n",
            "Epoch: 82/400, train_loss: 0.0100, time:0m 17s\n",
            "Epoch: 83/400, train_loss: 0.0100, time:0m 17s\n",
            "Epoch: 84/400, train_loss: 0.0100, time:0m 17s\n",
            "Epoch: 85/400, train_loss: 0.0103, time:0m 17s\n",
            "Epoch: 86/400, train_loss: 0.0103, time:0m 17s\n",
            "Epoch: 87/400, train_loss: 0.0103, time:0m 17s\n",
            "Epoch: 88/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 89/400, train_loss: 0.0105, time:0m 17s\n",
            "Epoch: 90/400, train_loss: 0.0104, time:0m 17s\n",
            "Epoch: 91/400, train_loss: 0.0102, time:0m 17s\n",
            "Epoch: 92/400, train_loss: 0.0103, time:0m 17s\n",
            "Epoch: 93/400, train_loss: 0.0101, time:0m 17s\n",
            "Epoch: 94/400, train_loss: 0.0101, time:0m 17s\n",
            "Epoch: 95/400, train_loss: 0.0099, time:0m 17s\n",
            "Epoch: 96/400, train_loss: 0.0101, time:0m 17s\n",
            "Epoch: 97/400, train_loss: 0.0100, time:0m 17s\n",
            "Epoch: 98/400, train_loss: 0.0103, time:0m 17s\n",
            "Epoch: 99/400, train_loss: 0.0100, time:0m 17s\n",
            "Epoch: 100/400, train_loss: 0.0099, time:0m 17s\n",
            "Epoch: 101/400, train_loss: 0.0102, time:0m 17s\n",
            "Epoch: 102/400, train_loss: 0.0100, time:0m 17s\n",
            "Epoch: 103/400, train_loss: 0.0101, time:0m 17s\n",
            "Epoch: 104/400, train_loss: 0.0100, time:0m 17s\n",
            "Epoch: 105/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 106/400, train_loss: 0.0102, time:0m 17s\n",
            "Epoch: 107/400, train_loss: 0.0101, time:0m 17s\n",
            "Epoch: 108/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 109/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 110/400, train_loss: 0.0099, time:0m 17s\n",
            "Epoch: 111/400, train_loss: 0.0102, time:0m 17s\n",
            "Epoch: 112/400, train_loss: 0.0101, time:0m 17s\n",
            "Epoch: 113/400, train_loss: 0.0099, time:0m 17s\n",
            "Epoch: 114/400, train_loss: 0.0100, time:0m 17s\n",
            "Epoch: 115/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 116/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 117/400, train_loss: 0.0100, time:0m 17s\n",
            "Epoch: 118/400, train_loss: 0.0099, time:0m 17s\n",
            "Epoch: 119/400, train_loss: 0.0100, time:0m 17s\n",
            "Epoch: 120/400, train_loss: 0.0099, time:0m 17s\n",
            "Epoch: 121/400, train_loss: 0.0100, time:0m 17s\n",
            "Epoch: 122/400, train_loss: 0.0099, time:0m 17s\n",
            "Epoch: 123/400, train_loss: 0.0099, time:0m 17s\n",
            "Epoch: 124/400, train_loss: 0.0100, time:0m 18s\n",
            "Epoch: 125/400, train_loss: 0.0100, time:0m 18s\n",
            "Epoch: 126/400, train_loss: 0.0098, time:0m 18s\n",
            "Epoch: 127/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 128/400, train_loss: 0.0102, time:0m 17s\n",
            "Epoch: 129/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 130/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 131/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 132/400, train_loss: 0.0102, time:0m 17s\n",
            "Epoch: 133/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 134/400, train_loss: 0.0101, time:0m 17s\n",
            "Epoch: 135/400, train_loss: 0.0099, time:0m 17s\n",
            "Epoch: 136/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 137/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 138/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 139/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 140/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 141/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 142/400, train_loss: 0.0099, time:0m 17s\n",
            "Epoch: 143/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 144/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 145/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 146/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 147/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 148/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 149/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 150/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 151/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 152/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 153/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 154/400, train_loss: 0.0094, time:0m 17s\n",
            "Epoch: 155/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 156/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 157/400, train_loss: 0.0099, time:0m 17s\n",
            "Epoch: 158/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 159/400, train_loss: 0.0102, time:0m 17s\n",
            "Epoch: 160/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 161/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 162/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 163/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 164/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 165/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 166/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 167/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 168/400, train_loss: 0.0099, time:0m 17s\n",
            "Epoch: 169/400, train_loss: 0.0099, time:0m 17s\n",
            "Epoch: 170/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 171/400, train_loss: 0.0099, time:0m 17s\n",
            "Epoch: 172/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 173/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 174/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 175/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 176/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 177/400, train_loss: 0.0092, time:0m 17s\n",
            "Epoch: 178/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 179/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 180/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 181/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 182/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 183/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 184/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 185/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 186/400, train_loss: 0.0094, time:0m 17s\n",
            "Epoch: 187/400, train_loss: 0.0094, time:0m 17s\n",
            "Epoch: 188/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 189/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 190/400, train_loss: 0.0094, time:0m 17s\n",
            "Epoch: 191/400, train_loss: 0.0094, time:0m 17s\n",
            "Epoch: 192/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 193/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 194/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 195/400, train_loss: 0.0098, time:0m 17s\n",
            "Epoch: 196/400, train_loss: 0.0094, time:0m 17s\n",
            "Epoch: 197/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 198/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 199/400, train_loss: 0.0094, time:0m 17s\n",
            "Epoch: 200/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 201/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 202/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 203/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 204/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 205/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 206/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 207/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 208/400, train_loss: 0.0094, time:0m 17s\n",
            "Epoch: 209/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 210/400, train_loss: 0.0092, time:0m 17s\n",
            "Epoch: 211/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 212/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 213/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 214/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 215/400, train_loss: 0.0094, time:0m 17s\n",
            "Epoch: 216/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 217/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 218/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 219/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 220/400, train_loss: 0.0094, time:0m 17s\n",
            "Epoch: 221/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 222/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 223/400, train_loss: 0.0092, time:0m 17s\n",
            "Epoch: 224/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 225/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 226/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 227/400, train_loss: 0.0097, time:0m 17s\n",
            "Epoch: 228/400, train_loss: 0.0092, time:0m 17s\n",
            "Epoch: 229/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 230/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 231/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 232/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 233/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 234/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 235/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 236/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 237/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 238/400, train_loss: 0.0094, time:0m 17s\n",
            "Epoch: 239/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 240/400, train_loss: 0.0096, time:0m 17s\n",
            "Epoch: 241/400, train_loss: 0.0094, time:0m 17s\n",
            "Epoch: 242/400, train_loss: 0.0089, time:0m 17s\n",
            "Epoch: 243/400, train_loss: 0.0092, time:0m 17s\n",
            "Epoch: 244/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 245/400, train_loss: 0.0092, time:0m 17s\n",
            "Epoch: 246/400, train_loss: 0.0092, time:0m 17s\n",
            "Epoch: 247/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 248/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 249/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 250/400, train_loss: 0.0089, time:0m 17s\n",
            "Epoch: 251/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 252/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 253/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 254/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 255/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 256/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 257/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 258/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 259/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 260/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 261/400, train_loss: 0.0089, time:0m 17s\n",
            "Epoch: 262/400, train_loss: 0.0095, time:0m 17s\n",
            "Epoch: 263/400, train_loss: 0.0092, time:0m 17s\n",
            "Epoch: 264/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 265/400, train_loss: 0.0089, time:0m 17s\n",
            "Epoch: 266/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 267/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 268/400, train_loss: 0.0093, time:0m 17s\n",
            "Epoch: 269/400, train_loss: 0.0088, time:0m 17s\n",
            "Epoch: 270/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 271/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 272/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 273/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 274/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 275/400, train_loss: 0.0084, time:0m 17s\n",
            "Epoch: 276/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 277/400, train_loss: 0.0089, time:0m 17s\n",
            "Epoch: 278/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 279/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 280/400, train_loss: 0.0088, time:0m 17s\n",
            "Epoch: 281/400, train_loss: 0.0088, time:0m 17s\n",
            "Epoch: 282/400, train_loss: 0.0089, time:0m 17s\n",
            "Epoch: 283/400, train_loss: 0.0089, time:0m 17s\n",
            "Epoch: 284/400, train_loss: 0.0089, time:0m 17s\n",
            "Epoch: 285/400, train_loss: 0.0088, time:0m 17s\n",
            "Epoch: 286/400, train_loss: 0.0086, time:0m 17s\n",
            "Epoch: 287/400, train_loss: 0.0085, time:0m 17s\n",
            "Epoch: 288/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 289/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 290/400, train_loss: 0.0088, time:0m 17s\n",
            "Epoch: 291/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 292/400, train_loss: 0.0086, time:0m 17s\n",
            "Epoch: 293/400, train_loss: 0.0091, time:0m 17s\n",
            "Epoch: 294/400, train_loss: 0.0089, time:0m 17s\n",
            "Epoch: 295/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 296/400, train_loss: 0.0081, time:0m 17s\n",
            "Epoch: 297/400, train_loss: 0.0088, time:0m 17s\n",
            "Epoch: 298/400, train_loss: 0.0086, time:0m 17s\n",
            "Epoch: 299/400, train_loss: 0.0086, time:0m 17s\n",
            "Epoch: 300/400, train_loss: 0.0087, time:0m 17s\n",
            "Epoch: 301/400, train_loss: 0.0087, time:0m 17s\n",
            "Epoch: 302/400, train_loss: 0.0089, time:0m 17s\n",
            "Epoch: 303/400, train_loss: 0.0084, time:0m 17s\n",
            "Epoch: 304/400, train_loss: 0.0088, time:0m 17s\n",
            "Epoch: 305/400, train_loss: 0.0086, time:0m 17s\n",
            "Epoch: 306/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 307/400, train_loss: 0.0085, time:0m 17s\n",
            "Epoch: 308/400, train_loss: 0.0090, time:0m 17s\n",
            "Epoch: 309/400, train_loss: 0.0085, time:0m 17s\n",
            "Epoch: 310/400, train_loss: 0.0088, time:0m 17s\n",
            "Epoch: 311/400, train_loss: 0.0084, time:0m 17s\n",
            "Epoch: 312/400, train_loss: 0.0088, time:0m 17s\n",
            "Epoch: 313/400, train_loss: 0.0086, time:0m 17s\n",
            "Epoch: 314/400, train_loss: 0.0086, time:0m 17s\n",
            "Epoch: 315/400, train_loss: 0.0087, time:0m 17s\n",
            "Epoch: 316/400, train_loss: 0.0087, time:0m 17s\n",
            "Epoch: 317/400, train_loss: 0.0088, time:0m 17s\n",
            "Epoch: 318/400, train_loss: 0.0086, time:0m 17s\n",
            "Epoch: 319/400, train_loss: 0.0086, time:0m 17s\n",
            "Epoch: 320/400, train_loss: 0.0086, time:0m 17s\n",
            "Epoch: 321/400, train_loss: 0.0088, time:0m 17s\n",
            "Epoch: 322/400, train_loss: 0.0085, time:0m 17s\n",
            "Epoch: 323/400, train_loss: 0.0089, time:0m 17s\n",
            "Epoch: 324/400, train_loss: 0.0086, time:0m 17s\n",
            "Epoch: 325/400, train_loss: 0.0085, time:0m 17s\n",
            "Epoch: 326/400, train_loss: 0.0086, time:0m 17s\n",
            "Epoch: 327/400, train_loss: 0.0088, time:0m 17s\n",
            "Epoch: 328/400, train_loss: 0.0084, time:0m 17s\n",
            "Epoch: 329/400, train_loss: 0.0084, time:0m 17s\n",
            "Epoch: 330/400, train_loss: 0.0083, time:0m 17s\n",
            "Epoch: 331/400, train_loss: 0.0083, time:0m 17s\n",
            "Epoch: 332/400, train_loss: 0.0082, time:0m 17s\n",
            "Epoch: 333/400, train_loss: 0.0089, time:0m 17s\n",
            "Epoch: 334/400, train_loss: 0.0082, time:0m 17s\n",
            "Epoch: 335/400, train_loss: 0.0086, time:0m 17s\n",
            "Epoch: 336/400, train_loss: 0.0084, time:0m 17s\n",
            "Epoch: 337/400, train_loss: 0.0085, time:0m 17s\n",
            "Epoch: 338/400, train_loss: 0.0084, time:0m 17s\n",
            "Epoch: 339/400, train_loss: 0.0084, time:0m 17s\n",
            "Epoch: 340/400, train_loss: 0.0082, time:0m 17s\n",
            "Epoch: 341/400, train_loss: 0.0081, time:0m 17s\n",
            "Epoch: 342/400, train_loss: 0.0083, time:0m 17s\n",
            "Epoch: 343/400, train_loss: 0.0083, time:0m 17s\n",
            "Epoch: 344/400, train_loss: 0.0082, time:0m 17s\n",
            "Epoch: 345/400, train_loss: 0.0083, time:0m 17s\n",
            "Epoch: 346/400, train_loss: 0.0086, time:0m 17s\n",
            "Epoch: 347/400, train_loss: 0.0084, time:0m 17s\n",
            "Epoch: 348/400, train_loss: 0.0082, time:0m 17s\n",
            "Epoch: 349/400, train_loss: 0.0077, time:0m 17s\n",
            "Epoch: 350/400, train_loss: 0.0083, time:0m 17s\n",
            "Epoch: 351/400, train_loss: 0.0080, time:0m 17s\n",
            "Epoch: 352/400, train_loss: 0.0086, time:0m 17s\n",
            "Epoch: 353/400, train_loss: 0.0082, time:0m 17s\n",
            "Epoch: 354/400, train_loss: 0.0082, time:0m 17s\n",
            "Epoch: 355/400, train_loss: 0.0083, time:0m 17s\n",
            "Epoch: 356/400, train_loss: 0.0081, time:0m 17s\n",
            "Epoch: 357/400, train_loss: 0.0088, time:0m 17s\n",
            "Epoch: 358/400, train_loss: 0.0083, time:0m 17s\n",
            "Epoch: 359/400, train_loss: 0.0082, time:0m 17s\n",
            "Epoch: 360/400, train_loss: 0.0084, time:0m 17s\n",
            "Epoch: 361/400, train_loss: 0.0081, time:0m 17s\n",
            "Epoch: 362/400, train_loss: 0.0085, time:0m 17s\n",
            "Epoch: 363/400, train_loss: 0.0082, time:0m 17s\n",
            "Epoch: 364/400, train_loss: 0.0081, time:0m 17s\n",
            "Epoch: 365/400, train_loss: 0.0083, time:0m 17s\n",
            "Epoch: 366/400, train_loss: 0.0084, time:0m 17s\n",
            "Epoch: 367/400, train_loss: 0.0082, time:0m 17s\n",
            "Epoch: 368/400, train_loss: 0.0084, time:0m 17s\n",
            "Epoch: 369/400, train_loss: 0.0080, time:0m 17s\n",
            "Epoch: 370/400, train_loss: 0.0079, time:0m 17s\n",
            "Epoch: 371/400, train_loss: 0.0081, time:0m 17s\n",
            "Epoch: 372/400, train_loss: 0.0079, time:0m 17s\n",
            "Epoch: 373/400, train_loss: 0.0082, time:0m 17s\n",
            "Epoch: 374/400, train_loss: 0.0083, time:0m 17s\n",
            "Epoch: 375/400, train_loss: 0.0084, time:0m 17s\n",
            "Epoch: 376/400, train_loss: 0.0083, time:0m 17s\n",
            "Epoch: 377/400, train_loss: 0.0081, time:0m 17s\n",
            "Epoch: 378/400, train_loss: 0.0082, time:0m 17s\n",
            "Epoch: 379/400, train_loss: 0.0083, time:0m 17s\n",
            "Epoch: 380/400, train_loss: 0.0080, time:0m 17s\n",
            "Epoch: 381/400, train_loss: 0.0083, time:0m 17s\n",
            "Epoch: 382/400, train_loss: 0.0081, time:0m 17s\n",
            "Epoch: 383/400, train_loss: 0.0082, time:0m 17s\n",
            "Epoch: 384/400, train_loss: 0.0078, time:0m 17s\n",
            "Epoch: 385/400, train_loss: 0.0080, time:0m 17s\n",
            "Epoch: 386/400, train_loss: 0.0082, time:0m 17s\n",
            "Epoch: 387/400, train_loss: 0.0083, time:0m 17s\n",
            "Epoch: 388/400, train_loss: 0.0077, time:0m 17s\n",
            "Epoch: 389/400, train_loss: 0.0081, time:0m 17s\n",
            "Epoch: 390/400, train_loss: 0.0082, time:0m 17s\n",
            "Epoch: 391/400, train_loss: 0.0083, time:0m 17s\n",
            "Epoch: 392/400, train_loss: 0.0083, time:0m 17s\n",
            "Epoch: 393/400, train_loss: 0.0079, time:0m 17s\n",
            "Epoch: 394/400, train_loss: 0.0084, time:0m 17s\n",
            "Epoch: 395/400, train_loss: 0.0080, time:0m 17s\n",
            "Epoch: 396/400, train_loss: 0.0078, time:0m 17s\n",
            "Epoch: 397/400, train_loss: 0.0084, time:0m 17s\n",
            "Epoch: 398/400, train_loss: 0.0082, time:0m 17s\n",
            "Epoch: 399/400, train_loss: 0.0079, time:0m 17s\n",
            "Epoch: 400/400, train_loss: 0.0081, time:0m 17s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# save train model\n",
        "path = os.path.join(data_dir, 'mobilenetv3_small_tue13.pth')\n",
        "torch.save(mobilenetv3_small.state_dict(), path)"
      ],
      "metadata": {
        "id": "-2D77yQwsKPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(Dataset): # 커스텀 데이터셋 구성을 위해 Dataset 상속 후 __init__, __len__, __getitem__ 오버라이딩\n",
        "    def __init__(self, initial_data, transform=None):\n",
        "        self.feature = initial_data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.feature)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.feature[idx]\n",
        "        if self.transform: # transform 있으면 적용\n",
        "            sample = self.transform(sample)\n",
        "        return sample\n",
        "\n",
        "test = TestDataset(initial_data=testset, transform=transforms.Compose([ # 위의 코드와는 다르게 transform이 파라미터로 들어감\n",
        "                                               transforms.ToTensor(),\n",
        "                                              #  transforms.Resize(256, antialias=False),\n",
        "                                               transforms.Normalize(*config['Cifar100_stats']), # mean = Cifar10_stats[0], std = Cifar10_stats[1]\n",
        "                                           ]))\n",
        "\n",
        "test_loader = DataLoader(test, batch_size = config['batch_size'], shuffle = False, num_workers = config['worker'])"
      ],
      "metadata": {
        "id": "qNiZDqpQ1G3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickleshare import Path\n",
        "import csv\n",
        "path = os.path.join(data_dir, 'tue13.csv')\n",
        "#Testing Accuracy\n",
        "correct = 0\n",
        "data = [[\"id_idx\", \"label\"]]\n",
        "with torch.no_grad():\n",
        "    for i, test_data in enumerate(test_loader):\n",
        "        images = test_data.to(device)\n",
        "        outputs = mobilenetv3_small(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        for index, batch_data in enumerate(predicted):\n",
        "          data.append([i*256 + index, batch_data.item()])\n",
        "\n",
        "    with open(path, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # 데이터를 CSV 파일로 쓰기\n",
        "        for row in data:\n",
        "            writer.writerow(row)"
      ],
      "metadata": {
        "id": "Rvge46JIXjaR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}